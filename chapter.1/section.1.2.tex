%! TEX root = main.tex
\subsection*{Data-driven and data-free physics-informed neural networks (PINNs)}

To our knowledge, PINNs is an ambiguous term with no authoritative definitions.
It is an umbrella term that includes all learning types that involve using PDEs in loss functions.

When Raissi et al. \cite{raissi_physics-informed_2017,raissi_physics_2017,raissi_physics-informed_2019} coined the term PINNs, they only distinguished two application types: {\it data-driven solutions of nonlinear partial differential equations} and {\it data-driven discovery of nonlinear partial differential equations}.
These two terms actually refer to forward and inverse problems.
In the same work, the term {\it data} in {\it data-driven} does not necessarily mean observed data (either experimental or computational).
For example, the test case of Burger's equation in section 2.1 of \cite{raissi_physics-informed_2017} does not use any data from experiments or other numerical solvers.
The {\it data} in this example refers to the values of boundary conditions (BCs) and initial conditions (ICs) at selected collocation points.
As BCs and ICs are given and are part of PDEs' specifications, from our viewpoint, these data do not carry the same meaning as those in section \ref{sec:overview} of this thesis.
The mentioned example of Burger's equation in the work of Raissi et al. should be classified as a data-free approach per the description in section \ref{sec:overview}.

Under the classification in section \ref{sec:overview}, the work of Raissi et al. is actually data-driven surrogate modeling with additional loss terms from governing PDEs.
Adding PDEs to the loss functions helps regulate or constrain the predictions of unseen data.
For example, suppose we have observed data at time $t_1$, $t_2$, $\cdots$, $t_{N_t}$ and spatial coordinates $\vec{x}_1$, $\vec{x}_2$, $\cdots$, $\vec{x}_{N_x}$. 
Conventional data-driven approaches usually have inferior prediction accuracy for points not among these spatial-temporal coordinates.
Especially because the fluid flow is a sensitive and unstable dynamical system, the predictions for $t < t_1$ and $t > t_{N_t}$ may deviate from the physics even qualitatively.
Adding PDEs to the training objectives helps in such situations.

In the remaining of this thesis, following the description in section \ref{sec:overview}, we will use {\it data-driven} to denote the use of experimental data or data generated by numerical solvers.
{\it Data-free} refers to situations when no experimental or extra numerical data are involved. 

Raissi et al. did not distinguish the difference between {\it data-driven} and {\it data-free} as we do.
In PINNs, the treatment of observed data is the same as that of Dirichlet BCs and ICs.
Under their PINNs, it is even possible to have no BCs and ICs.
One can use only observed data and governing PDEs to train the neural networks and build surrogate models.
Under this consideration, data-free PINNs, in our terminology, are just special cases of data-driven PINNs when no external data exists.
Hence, Raissi et al. used {\it data-driven} throughout their work, though some test cases are {\it data-free} in our classification.

When no observed data exist, and when BCs and ICs are used in training, the PINNs become the same as the one proposed by Dissanayake and Phan-Thien \cite{dissanayake_neural-network-based_1994}.
Moreover, the goal of PINNs is changed from constructing surrogate models to solving PDEs.
If the goal is to solve PDEs rather than create a surrogate model, most importantly, the BCs and ICs of PDEs carry different meanings than the observed data.
Observed data are allowed to and usually do have noise and fluctuations.
On the other hand, BCs and ICs are parts of PDEs; they should be satisfied strictly.
So while we allow the model predictions to deviate from observations to some degree, we want the model predictions on both spatial and temporal boundaries to match the BCs and ICs exactly.
In terms of optimization, as we will discuss later, BCs and ICs are hard constraints, and observed data are simply objective.
Therefore, we believe it is necessary to exclude BCs and ICs from {\it data} and distinguish between data-free and data-driven PINNs.

Most articles on PINNs in recent years do not distinguish the two, either.
For example, references \cite{wang_understanding_2021,krishnapriyan_characterizing_2021,wang_when_2022} are about data-free PINNs and their capability to solve PDEs.
On the contrary, reference \cite{cai_physics-informed_2021} focuses on data-driven PINNs, and all test cases in this work are applications of data-driven PINNs.

Data-free PINNs are numerical methods for solving PDE/ODE.
They are potential alternatives to mainstream CFD solvers, such as finite-difference, finite-volume, and finite-element methods.
In this work, we will focus on data-free PINNs.
Prior to the work of Raissi et al., there was no specific name for methods that solve PDEs with neural networks.
Therefore, in our work, we use data-free PINNs to refer to all methods of this type.

\subsection*{Literature review of data-free PINNs}

As mentioned, data-free PINNs can be traced back to at least 1994 \cite{dissanayake_neural-network-based_1994}.
In earlier studies, regardless of the networks' architectures, the derivatives of the model outputs with respect to model inputs were analytical.
It is the primary reason that these studies could only use simple network architectures and smooth nonlinear activation functions. 
Activation functions are limited to those with smooth derivatives and simple expressions such as the sigmoid function.
For example, Dissanayake and Phan-Thien \cite{dissanayake_neural-network-based_1994} used MLP networks with 2-hidden layers, up to 10 neurons per layer, and the sigmoid function.
Lagaris \cite{lagaris_artificial_1998} used an MLP network with only one hidden layer and 10 neurons per layer.
Mai-Duy and Tran-Cong \cite{mai-duy_numerical_2001}, Li et al. \cite{li_numerical_2003}, Demirkaya et al. \cite{demirkaya_direct_2008}, and Wang et al. \cite{wang_meshless_2015} used RBF networks, which can be seen as a single-layer MLP network of RBF neurons and are easy to obtain the analytical derivatives.
McFall and his coworkers \cite{McFall2009,mcfall_solving_2010} used a single-layer MLP with up to 50 neurons.

We believe the limitation on the network architectures also limited the test cases used in these studies.
Among these earlier studies, only \cite{demirkaya_direct_2008,mcfall_solving_2010,wang_meshless_2015} showed applications to the Navier-Stokes equations for simple simulations (mostly 2D Poiseuille flow). 
Other works mentioned here tested their proposed methods with simpler PDEs (e.g., 1D or 2D Poisson equations) and implied the possibility of applications in flow problems.

More recently, automatic differentiation has been explicitly mentioned in the literature, for example, \cite{berg_unified_2018,sirignano_dgm:_2018,dockhorn_discussion_2019,raissi_physics-informed_2017,raissi_physics-informed_2019,jin_nsfnets_2020,lu_deepxde:_2021,cai_physics-informed_2021}.
Using automatic differentiation makes it possible to approximate PDE solutions with more complicated neural network architectures.
Sirignano and Spiliopoulos \cite{sirignano_dgm:_2018} proposed an architecture similar to long short-term networks (LSTMs \cite{hochreiter_long_1997}).
Due to the recurrent nature of LSTMs, it is not practical, if not impossible, to obtain the analytical forms of the derivatives for this model either through manual derivation or computer symbolic derivations.
Studies that used MLP networks, on the other hand, increased the complexity of the model by increasing the number of hidden layers and neurons.
Architectures with up to 10 hidden layers and 300 neurons per layer were used in the work of Jin et al. \cite{jin_nsfnets_2020}.

Another critical component for data-free PINNs is the training process (a.k.a. model fitting).
The free parameters are determined by optimizing/minimizing the PDE residuals.
However, PDE solutions are bounded to ICs or/and BCs.
One can deem ICs and BCs as hard constraints when optimizing PDE residuals.
Earlier works of data-free PINNs converted this constrained problem to an unconstrained one by treating an approximate PDE solution as a superposition of a nonhomogeneous-BC function and a solution under homogeneous BCs.
The nonhomogeneous-BC function is defined to meet the boundary conditions and initial conditions at collocation points on spatial-temporal boundaries.
The solution under homogeneous BCs is further decoupled into the multiplication of a length function and a neural network. 
The length function guarantees that the neural network outputs always diminish on Dirichlet boundaries and that the derivatives in normal directions always diminish on Neumann boundaries.
Lagaris \cite{lagaris_artificial_1998} showed an example of how to design such a length function and the nonhomogeneous-BC function for rectangular computational domains.
McFall and Mahan \cite{McFall2009} further proposed using thin plane splines (TPSs) for both the nonhomogeneous-BC functions and the length functions in non-rectangular domains.
They tested the proposed approach on the steady Navier-Stokes equations (steady boundary layer simulations and 2D Poiseuille flow). 
Note that TPSs are considered a type of RBFs, and one can classify the TPS method as an RBF neural network.
That says, McFall and his coworkers' work can be extended to using other neural network architectures to model the length functions and nonhomogeneous-BC functions.
These studies, however, did not investigate applications in initial-boundary value problems.
Moreover, it has not been proved in theory nor extensively shown in experiments that claimed-to-be general length functions and nonhomogeneous-BC functions (like TPSs) can be applied to other PDEs and applications.
These functions may be problem-specific.

If we treat ICs and BCs as soft constraints, the training becomes a multi-objective optimization with each condition being an objective (i.e., a loss function).
A straightforward approach is to treat the residuals from BCs and ICs as losses and use the weighted sum of all losses to obtain a single loss. 
An alternative view of the weighted sum of losses is to deem it a special case of penalty function methods in constrained optimization.
The penalty functions are the residuals of ICs and BCs.
Like other penalty method functions, this approach is considered to converge slower \cite{rudd_constrained_2014}.
However, most recent studies of PINNs (both data-free and data-driven) adopted this approach.
Using a weighted sum of losses offers simplicity for mathematical expressions and programming.

Constant weights of one can be used in all terms in the weighted sum of losses (i.e., simply summing all losses), as seen in \cite{lagaris_artificial_1998,sirignano_dgm:_2018,dockhorn_discussion_2019}.
Although constant weights other than one can be used, no established guideline exists to help decide proper values.
Jin et al. \cite{jin_nsfnets_2020} and Wang et al. \cite{wang_understanding_2021} proposed annealing loss algorithms to change the weight of each loss term during training dynamically.
These adaptive weights are determined using the magnitudes of losses' gradients with respect to model parameters.
It also involves the moving average of the current and previous iterations' weights.

To resolve the issues of problem-specific length and nonhomogeneous-BC functions in the first approach or the slow convergence in the second approach, Rudd and his coworkers \cite{rudd_constrained_2014,rudd_constrained_2015} solved the optimization problem in data-free PINNs with a constrained backpropagation method.
The hard constraints from BCs and ICs are satisfied during the training in this method.
This method, however, has not been widely tested and adopted by other researchers.

After converting to unconstrained problems, the actual optimization is usually done with methods based on gradient descent and in a batched approach. 
Adam \cite{kingma_adam_2017} is the most popular optimizer nowadays in the literature on PINNs.
Nevertheless, Krishnapriyan et al. \cite{krishnapriyan_characterizing_2021} reported that L-BFGS performed significantly better than Adam for unknown reasons.
Jin et al. \cite{jin_nsfnets_2020} also used L-BFGS to finalize the training.
However, even though L-BFGS needs much less computer memory than its origin, BFGS, it may still be impractical to use it in real-world problems due to the problem sizes.
Besides, to our knowledge, batch training is not well established for L-BFGS.
L-BFGS memorizes gradients from several past iterations.
So when the loss function's hypersurface changes at each iteration due to using different batches of data, the stored information of past gradients in L-BFGS may be problematic at the current iteration.

As for the nonlinear activation functions, to our best knowledge, it is still an unexplored area in data-free PINNs.
Most studies used either the sigmoid function or the hyperbolic tangent function without giving solid justifications.
Li et al. \cite{li_integration_2010} in 2010 proposed using wavelet functions for the activation.
However, their method has not received wide attention so far.
Neither did they justify why or how wavelet functions were better choices.
% vim:ft=tex