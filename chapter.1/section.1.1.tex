%! TEX root = main.tex

CFD (computational fluid dynamics) and other applications of numerical PDEs (partial differential equations) were once the primary reason for the advance in HPC (high-performance computing) hardware and programming tools.
However, deep learning applications have recently become the major player that pushes the boundaries of these computing technologies.
For example, Google developed TPU (tensor processing unit), TensorFlow, and Colaboratory for deep learning.
NVIDIA has been promoting its recent-generation GPUs (graphical processing units) for deep learning since 2015~\cite{buck_nvidias_2015}.
Even supercomputing centers, such as those managed by the United States Department of Energy, shifted their focus to deep learning for their next-generation hardware~\cite{us_department_of_energy_argonne_2019}.
As hardware and programming tools are the two vital resources for CFD practitioners, it is inevitable for CFD practitioners and the industry to adopt deep learning for practical engineering in fluid dynamics.

We roughly categorize deep learning applications in CFD into two types: data-driven and data-free.

\subsection*{Data-driven learning in CFD}

Data-driven deep learning may be considered a much more complicated version of linear regression.
Fundamentally, we want to find a mathematical model that best fits some observed data.
However, in deep learning, the model is not linear anymore.
The models are now deep neural networks and have much more free parameters (compared to only two parameters in linear models) that must be determined by regression.
The regression process---also called training in deep learning or model fitting in engineering---determines the values of free parameters by minimizing the distance between observations and predictions (i.e., the outputs of the models).
Once the free parameters are determined, the model can be used as an interpolator to reconstruct the missing data or an extrapolator to predict quantities outside the boundary of observations.

As data fitting has long existed in engineering, a straightforward application of deep learning in CFD is data-driven surrogate modeling.
Practitioners use deep neural networks to model the desired flow physics in such an application.
The observations that the models fit against may be experimental or computational data.
The fitted models give quick predictions in situations where solving governing equations is not feasible.
For example, flight control systems need such a model to quickly predict how lift and drag forces respond to changes in speed, angle of attack, or environmental factors. 
Surrogate modeling with deep learning in aerodynamics has been studied since at least 1993 \cite{linse_identification_1993, faller_unsteady_1997}.
Researchers have also recently used data-driven surrogate modeling to model the closure equations for turbulence simulations or flow data reconstruction (e.g., \cite{Tracey2015,milano_neural_2002}).

Data-driven learning in CFD is well established and has a wide presence in practical engineering.
Recent studies in this area focused on improving accuracy, training efficiency, and generalizability (i.e., the capability to predict unseen data accurately).

\subsection*{Data-free learning in CFD}

Given the strong connection between CFD and PDEs, solving PDEs using neural networks and without given data is another popular research topic in recent years.
Simply put (see chapter \ref{chap:pinn} for details), a neural network model is used to approximate the solutions of PDEs.
The model is a mathematical model, so the derivatives of the model outputs with respect to model inputs can be obtained analytically or through automatic differentiation \cite{griewank_automatic_1988}.
Once having mathematical expressions of the derivatives, we substitute them into the differential equations and get the residuals. 
Finally, we determine the free parameters in the model by optimizing/minimizing the residuals.
In other words, the model fits against PDEs rather than observed data; hence it is called data-free.
The workflow is similar to least-square finite element methods (LSFEM \cite{jiang_least-squares_1998}) or collocation spectral methods \cite{trefethen_spectral_2000}.
We will develop a brief comparison of the methods in section \ref{sec:pinn-diff}.
Figure \ref{fig:pinn-workflow} shows a graphical illustration of applying this workflow to solve the Navier-Stokes equations. 

Though only becoming popular recently, solving PDEs with neural networks can be dated back to the early 90s.
If we limit ourselves to forward problems, for example, in 1994, Dissanayake and Phan-Thien \cite{dissanayake_neural-network-based_1994} proposed using multilayer perceptron (MLP) networks (also called fully-connected neural networks) as the approximate solutions to PDEs and using the PDEs as the loss functions for model fitting.
Lagaris et al., in 1998 \cite{lagaris_artificial_1998}, also proposed a similar approach but used auxiliary models to enforce boundary conditions.
McFall and his coworkers \cite{McFall2009,mcfall_solving_2010} further improved the enforcement of boundary conditions.
Other types of neural network models, such as RBF (radial basis function) neural networks, can be found in references \cite{li_numerical_2003,mai-duy_numerical_2001,demirkaya_direct_2008,li_integration_2010}.
On the other hand, Takeuchi and Kosugi \cite{takeuchi_neural_1994} rearranged a neural network as a finite-element method to solve both forward and inverse problems in electric field applications.
Other attempts before the 2010s that solved PDEs with neural networks and under a data-free setting can be found in reference \cite{yadav_introduction_2015}.

Since the mid-2010s, solving PDEs with neural networks suddenly became popular after the term {\it physics-informed neural networks} (PINNs) was coined by Raissi et al. \cite{raissi_physics-informed_2017}.
The core concept of PINNs is the same as what Dissanayake and Phan-Thien proposed in 1994.
Searching the keyword {\it physics-informed neural networks} with Google Scholar reveals a rapid growth from 60 results in 2017 to \num{3340} results in 2021.
We believe this growth of interest in solving PDEs with neural networks can be attributed to the advance in computing technologies (e.g., GPU technology) and the easy access to these technologies.
For example, on-demand cloud computing services make deep learning affordable for small research groups.
Furthermore, the robust Python ecosystem---and tools like TensorFlow and PyTorch---reduce the effort of code development. 
As developing code for PINNs has been made easy, it is natural for more researchers to devote their time to PINNs.

% vim:ft=tex
