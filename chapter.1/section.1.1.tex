%! TEX root = main.tex

While CFD (computational fluid dynamics) and other applications of numerical PDEs (partial differential equations) were once the major reason for the advance in HPC (high-performance computing) hardware and programming tools, the boundaries of these computing resources have been recently pushed by deep learning.
For example, Google developed TPU (tensor processing unit), TensorFlow, and Colaboratory with deep learning in mind.
NVIDIA has been promoting its recent-generation GPUs (graphical processing units) for deep learning since 2015~\cite{buck_nvidias_2015}.
Even supercomputing centers, such as those managed by the United States Department of Energy, shifted their focuses to deep learning for their next-generation hardware~\cite{us_department_of_energy_argonne_2019}.
As hardware and programming tools are the two vital resources for CFD practitioners, it is inevitable for CFD practitioners and the industry to adopt deep learning for practical engineering of fluid dynamics.

Applications of deep learning in CFD can be roughly categorized into two types: data-driven and data-free applications.

\subsection*{Data-driven learning in CFD}

Data-driven deep learning can be naively thought as a much more complicated version of linear regression.
Fundamentally, we want to find a mathematical model that best fits some observed data.
However, in deep learning, the model is not linear anymore.
The models are now deep neural networks and has much more free parameters (compared to only two parameters in linear models) that need to be determined during regression process.
The regression process, which is called training in deep learning context or called model fitting in engineering, determines the values of free parameters by minimize the distance between observations and predictions (i.e., the outputs of the models).
Once the free parameters are determined, the model can be used as an interpolator to reconstruct the missing data or an extrapolator to predict quantities outside the boundary of observations.

As data fitting has been long existed in engineering, a straightforward application of deep learning in CFD is data-driven surrogate modeling.
In such an application, practitioners use deep neural networks to model the desired flow physics.
The observations that the models fit against may be experimental or computational data.
The fitted models are used to give quick predictions in situations where solving governing equations are not feasible.
For example, flight control systems need such a model to quickly predict how lift and drag forces respond to change of speed, angle of attack, or environmental factors (wind, temperature, etc.). 
Surrogate modeling with deep learning in aerodynamics has been studied since as early as 1993 (\cite{linse_identification_1993, faller_unsteady_1997}).
More recently, data-driven surrogate modeling is also used to model the closure equations for turbulence simulations or flow data reconstruction (e.g., \cite{Tracey2015,milano_neural_2002}).

Data-driven learning in CFD is well established and has been applied to practical engineering.
Recent studies in this area focused on how to improve the accuracy, training efficiency, generalizability (i.e., the capability to predict unseen data accurately).

\subsection*{Data-free learning in CFD}

Given the strong connection between CFD and PDEs, solving PDEs using neural networks and without given data is another popular research topic in recent years.
To simply put (see chapter \ref{chap:pinn} for details), a neural network model is used to approximate the solutions of PDEs.
The model is a mathematical model, so the derivatives of the model outputs with respect to model inputs can be obtained analytically or through automatic differentiation \cite{griewank_automatic_1988}.
Once having mathematical expressions of the derivatives, we substitute them into the differential equations and get the residuals. 
Finally, we determine the free parameters in the model by optimize/minimize the residuals.
In other words, the model fits against PDEs rather than observed data, and hence it is called data-free.
The workflow is similar to least square finite element methods (LSFEM \cite{jiang_least-squares_1998}) or collocation spectral methods \cite{trefethen_spectral_2000}.
A brief comparison of the methods are described in section \ref{sec:pinn-diff}.
Figure \ref{fig:pinn-workflow} shows a graphical demonstration of applying this workflow to solve the Navier-Stokes equations. 

Though only been popular recently, solving PDEs with neural networks can be dated back to early 90s.
If we limit ourselves to forward problems, for example, in 1994, Dissanayake and Phan-Thien \cite{dissanayake_neural-network-based_1994} proposed using multilayer perceptron (MLP) networks (or called fully-connected neural networks) as the approximate solutions to PDEs and using the PDEs as the loss functions for model fitting.
Lagaris et al. in 1998 \cite{lagaris_artificial_1998} also proposed a similar approach but with auxiliary models to enforce boundary conditions.
McFall and his coworkers \cite{McFall2009,mcfall_solving_2010} further improved the enforcement of boundary conditions.
Other types of neural network models, such as RBF (radial basis function) neural networks can be found in references \cite{li_numerical_2003,mai-duy_numerical_2001,demirkaya_direct_2008}, and \cite{li_integration_2010}.
Takeuchi and Kosugi \cite{takeuchi_neural_1994}, on the other hand, rearranged a neural network as a finite-element method to solve both forward and inverse problems in electric field problems.
Other attempts prior to the 2010s that solved PDEs with neural networks and under a data-free setting can be found in references \cite{yadav_introduction_2015}.

Since mid-2010s, solving PDEs with neural networks suddenly became popular after the term {\it physics-informed neural networks} (PINNs) was coined by Raissi et al. \cite{raissi_physics-informed_2017}.
The core concept of PINNs is the same as what Dissanayake and Phan-Thien proposed in 1994.
Searching the keyword {\it physics-informed neural networks} with Google Scholar reveals a rapid growth from 60 results to 3,340 results in 2021.
We believe this growth of interests in solving PDEs with neural networks can be attributed to the advance in computing technologies (e.g., GPU technology) and the easy access to these technologies.
For example, on-demand cloud computing services make deep learning affordable for small research groups.
And powerful Python ecosystem together with tools like TensorFlow and PyTorch reduce effort of code development.
As developing code for PINNs has been made easy, it is natural for more researchers to devote their time in PINNs.

% vim:ft=tex