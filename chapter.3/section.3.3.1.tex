%! TEX root = main.tex
In iterative optimization methods that are derived from the gradient -descent method, the last step in each iteration is to update parameters with the corrected or uncorrected gradients of the loss with respect to model parameters:
\begin{equation}
    \vec{\theta}^{k+1} = \vec{\theta}^{k} - \eta\Phi(\nabla_{\Theta}r(\Theta=\vec{\theta}^k))
\end{equation}
where $\eta$ is a hyperparameter controlling the step size to move in the negative gradient direction on the hypersurface of $r(\Theta)$ in $\Theta$ space.
$\Phi(\nabla_{\Theta}r(\vec{\theta}^k))$ represents either a corrected or uncorrected gradient.
For example, $\Phi(\nabla_{\Theta}r(\vec{\theta}^k))=\nabla_{\Theta}r(\vec{\theta}^k)$ in the vanilla gradient-descent method.
And $\Phi(\nabla_{\Theta}r(\vec{\theta}^k)) = \left(1-\beta\right)\nabla_{\Theta}r(\vec{\theta}^k) + \beta \vec{g}^{k-1}$ for the gradient-descent with momentum, where $\vec{g}^{k-1}$ is the running average of $\nabla_{\Theta}r(\Theta)$ up to $k-1$-th iteration.
See \cite[Section~8.3]{goodfellow_deep_2016} for the details of these optimization methods.
As the iteration approaches a local minimum, the gradient approaches zero, and eventually the training converges to $\vec{\theta}$ when $\vec{\theta}^k=\vec{\theta}^{k-1}$

The step size $\eta$ is called {\it learning rate} in modern machine learning.
The configuration of the learning rate has always been an open question.
In earlier iterations, a large learning rate helps speed up the convergence.
However, in later iterations, when the training approach a local minimum, a large learning rate may make the step too big and miss the minimum.
So common practice is to have smaller and smaller learning rates for later iterations.
A way to adjust learning rate without human intervention is to use a learning rate scheduling based on the iteration counter.
For example, an exponential learning has a formula of
\begin{equation}
    \eta(k) = \eta_0 \times \gamma^k
\end{equation}
where $k$ denotes the $k$-th iteration, and $\gamma$ is a hyperparameter controlling the decaying rate.

Though a large learning rate may miss a local minimum, it sometimes also helps in escaping from a local minimum or a saddle point on the hypersurface of $r(\Theta)$.
With exponential learning rate adjustment, later iterations may lose the ability to find a new and better local minimum or lose the ability to quickly go over the plateaus surrounding saddle points.

Smith \cite{smith_cyclical_2017} proposed a cyclical learning rate adjusting strategy for general deep learning applications.
He found the cyclical learning rate increased the accuracy of a trained model and accelerated the convergence of the training.
In the cyclical learning rate, learning rate bounces back and forth between an upper and lower bounds.
The upper bound may be a constant or a value depending on the iteration counter.

The intuitive reason why cyclical learning rate works is that the difficulty of training deep neural networks is dominated by saddle points rather than poor local minimums \cite{dauphin_identifying_2014}.
The cyclical learning rate helps escape saddle points in later iterations by providing large learning rates at the upper bound.
And once escaping from the saddle points and in a convex region of a minimum, the small learning rates help lock the minimum.
The large learning rate is not supposed to have a strong effect after reaching a convex and locking a minimum as the gradients around a minimum are usually small.

In this work, we would like to try an exponential-range cyclical learning rate, also proposed in \cite{smith_cyclical_2017}.
Given a half-cycle size $N_c$ (that is, it takes $N_c$ iterations for the rate to go from the lower to the upper bound or from the upper to the lower bound), a counter $c$ indicating the current cycle, and $k$ indicating the current iteration, we have
\begin{equation}\label{eq:cyclical-learning-rate}
    \eta(k) = \eta_{low} + \max(0, 1-s)\times(\eta_{high}-\eta_{low})\times\gamma^k
\end{equation}
where $s \equiv \left\lvert \frac{k}{N_c} - 2c + 1\right\rvert$ and $0 \le s \le 1$.
Figure \ref{fig:cyclic-swa-tests-lr-hist} in chapter \ref{chap:pinn-cases} shows an example of how learning rate changes with iterations. 

Another training technique we would like to test together with the cyclical learning rate is the stochastic weight averaging (SWA) \cite{izmailov_averaging_2019}.
SWA basically averages the $\vec{\theta}^k$ from the last few iterations to get the final model parameters:
\begin{equation}
    \vec{\theta} = \frac{1}{N_{SWA}}\sum\nolimits_{i=N_k-N_{SWA}+1}^{N_k} \vec{\theta}^i
\end{equation}
where $N_{SWA}$ denotes the number of iterations to be used in the averaging, and $N_k$ represents the total iterations of the iterative optimization methods.

Izmailov et al. \cite{izmailov_averaging_2019} showed that SWA gave a better accuracy and generalizability of the trained models.
The intuitive mechanism behind SWA is that, on the hypersurface of $r(\Theta)$, the optimizer rarely finds exact minimum but usually circulating in the vicinity of that location.
The last few iterations at the end of training are composed of the coordinates surrounding the actual coordinates of the minimum loss on the hypersurface of $r(\Theta)$ in the $\Theta$ vector space.
Hence, an average of the $\vec{\theta}^k$ may give us the $\vec{\theta}$ of that sits at the center of these $\vec{\theta}^k$.

Another interpretation of SWA is through the combination of the hypersurfaces from different iterations.
Due to the batched training, the hypersurface of $r(\Theta)$ at each iteration is slightly different.
Hence, the $\vec{\theta}^k$ corresponding to the minimum at one iteration does not give the minimal loss and close-to-zero gradients at the next iteration.
SWA helps create a hypersurface with superposition of hypersurfaces at different iterations, and this new hypersurface has a flatter region surrounding the minimum.
This wide and flat region of minimum represents the generalizability of a trained model: how well the model can predict against inputs it never or rarely saw.
And an average of $\vec{\theta}^k$ sits at the center of this wide and flat region of minimum.

As Izmailov et al. proposed to use SWA together with the cyclical learning rate, we would like to include SWA into our benchmarks.