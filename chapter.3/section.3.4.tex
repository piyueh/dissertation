%! TEX root = main.tex
In this section, we would like to make a brief analogy between traditional numerical methods and PINNs.
When solving differential equations numerically, we can describe the solution workflows of most numerical methods with five stages:
\begin{enumerate}[nolistsep]
    \item Designing the approximate solution with undetermined parameters
    \item Choosing proper approximation for derivatives
    \item Obtaining the modified equations by substituting approximate derivatives into the differential equations, IC, and BCs
    \item Generating a system of linear/nonlinear algebraic equations
    \item Solving the system of equations
\end{enumerate}

For example, to solve $\frac{\diff^2 U(x)}{\diff x^2}=s(x)$, the most naive spectral method \cite{trefethen_spectral_2000} approximates the solution with $U(x)\approx G(x; \Theta)\equiv\sum\limits_{i=1}^{N}\theta_i\phi_i(x)$, where $\theta_i$ represents the free model parameters; $\phi_i(x)$ denotes a set of either polynomials, trigonometric functions, or complex exponentials; and $N$ is the number of terms in the approximation.
Next, approximating the 1st-order derivative $\frac{\diff U(x)}{\diff x}$ is straightforward--we can assume $\frac{\diff U(x)}{\diff x} \approx \pdiff{G(x; \Theta)}{x}=\sum\limits_{i=1}^{N}\theta_i \frac{\diff \phi_i(x)}{\diff x}$.
The 2nd-order derivative may follow the same workflow: $\frac{\diff^2 U(x)}{\diff x^2} \approx \frac{\partial^2 G(x; \Theta)}{\partial x^2}=\sum\limits_{i=1}^{N}\theta_i \frac{\diff^2 \phi_i(x)}{\diff x^2}$.
$\phi_i(x)$ is known, so the derivatives $\frac{\diff \phi_i(x)}{\diff x}$ and $\frac{\diff^2 \phi_i(x)}{\diff x^2}$ are analytical.

Substitute the approximate derivatives into the differential equation, we obtain the residual function in continuous space: $r(x; \Theta) \equiv \sum\limits_{i=1}^{N}\theta_i \frac{\diff^2 \phi_i(x)}{\diff x^2} - s(x)$.

Finally, to determine the actual values of $\theta_i$, one approach is to use $N$ distinct $x$ values at which $r(x; \Theta) = 0$.
This results in a system of linear equations: 
\begin{equation}\label{eq:spectral-linear-sys}
    \begin{bmatrix}
        \frac{\diff^2 \phi_1}{\diff x^2}(x_1) & \cdots & \frac{\diff^2 \phi_N}{\diff x^2}(x_1) \\
        \vdots & \ddots & \vdots \\
        \frac{\diff^2 \phi_1}{\diff x^2}(x_N) & \cdots & \frac{\diff^2 \phi_N}{\diff x^2}(x_N) \\
    \end{bmatrix}
    \begin{bmatrix}
        \theta_1 \\ \vdots \\ \theta_N
    \end{bmatrix}
    - 
    \begin{bmatrix}
        s(x_1) \\ \vdots \\ s(x_N)
    \end{bmatrix}
    = 0
\end{equation}
Solving this linear system determines the values of $\theta_i$ and conclude the solving workflow of this naive spectral method.
The obtained $\theta_i$ guarantees that the residuals are zero at least on the $N$ chosen $x$ coordinates.

Though this example uses a spectral method, the workflow also applies to many other numerical methods, such as finite difference methods, which can be reformatted as a form of spectral method.

Alternatively, some numerical methods solve $\theta_i$ through finding the values that minimize the square of the residual across the whole $x$ domain:
\begin{equation}\label{eq:least-square-fem}
    \begin{aligned}
        \begin{bmatrix}
            \theta_1 \\ \vdots \\ \theta_N
        \end{bmatrix}
        & =
        \operatorname*{arg\,min}\limits_{\theta_i}
        \int\limits_{x}\left[r(x; \Theta)\right]^2\diff x \\
        & =
        \operatorname*{arg\,min}\limits_{\theta_i}
        \int\limits_{x}\left[\sum\limits_{i=1}^{N}\theta_i \frac{\diff^2 \phi_i(x)}{\diff x^2} - s(x)\right]^2\diff x
    \end{aligned}
\end{equation}
However, in these numerical methods, the optimization is done by solving the zero-slope conditions directly:
\begin{equation}
    \pdiff{\left[r(x; \Theta)\right]^2}{\theta_1} = 
    \pdiff{\left[r(x; \Theta)\right]^2}{\theta_2} = 
    \cdots =
    \pdiff{\left[r(x; \Theta)\right]^2}{\theta_N} = 
    0
\end{equation}
which also results in a linear system:
\begin{equation}\label{eq:spetral-least-equare}
    \begin{bmatrix}
        \int\limits_{x}
        \frac{\diff^2 \phi_1}{\diff x^2}
        \frac{\diff^2 \phi_1}{\diff x^2}
        \diff x
        &
        \cdots
        &
        \int\limits_{x}
        \frac{\diff^2 \phi_N}{\diff x^2}
        \frac{\diff^2 \phi_1}{\diff x^2}
        \diff x \\
        \vdots & \ddots & \vdots \\
        \int\limits_{x}
        \frac{\diff^2 \phi_1}{\diff x^2}
        \frac{\diff^2 \phi_N}{\diff x^2}
        \diff x
        &
        \cdots
        &
        \int\limits_{x}
        \frac{\diff^2 \phi_N}{\diff x^2}
        \frac{\diff^2 \phi_N}{\diff x^2}
        \diff x \\
    \end{bmatrix}
    \begin{bmatrix}
        \theta_1 \\ \vdots \\ \theta_N
    \end{bmatrix}
    - 
    \begin{bmatrix}
        \int\limits_{x}s(x)\frac{\diff \phi_1}{\diff x}(x) \diff x \\
        \vdots \\
        \int\limits_{x}s(x)\frac{\diff \phi_1}{\diff x}(x) \diff x
    \end{bmatrix}
    = 0
\end{equation}
As $\phi_i$ is given, the integrals can be evaluated analytically.
Moreover, a proper choice of $\phi_i$ can make the coefficient matrix in \eqref{eq:spetral-least-equare} sparse, making it cheap and fast to solve.
Finally, $\Theta=\{\theta_1,\cdots\theta_N\}$ is determined.
In this approach, the obtained $\theta_i$ guarantees that the integrated residual over the whole domain is minimal, but it does not guarantee a zero residual unless $r(x; \Theta) = \sum\limits_{i=1}^{N}\theta_i \frac{\diff^2 \phi_i(x)}{\diff x^2} - s(x)$ has zero roots with respect to $\Theta$ for any given $x$.

If we replace the integral in \eqref{eq:least-square-fem} with a Monte-Carlo numerical integration:
\begin{equation}\label{eq:least-square-monte-carlo}
    \begin{aligned}
        \begin{bmatrix}
            \theta_1 \\ \vdots \\ \theta_N
        \end{bmatrix}
        & =
        \operatorname*{arg\,min}\limits_{\theta_i}
        \int\limits_{x}\left[r(x; \Theta)\right]^2\diff x \\
        &\approx
        \operatorname*{arg\,min}\limits_{\theta_i}
        \frac{L_x}{N}\sum\limits_{m=1}^{N}\left[r(x_m; \Theta)\right]^2 \\
        & =
        \operatorname*{arg\,min}\limits_{\theta_i}
        \sum\limits_{m=1}^{N}\left[\sum\limits_{i=1}^{N}\theta_i \frac{\diff^2 \phi_i(x_m)}{\diff x^2} - s(x_m)\right]^2
    \end{aligned}
\end{equation}
where $L_x$ is the computational domain, and $N$ is the number of random points sampled from the computational domain.
We can see that \eqref{eq:least-square-monte-carlo} is actually the loss function in PINNs (equations \eqref{eq:residual-norms} and \eqref{eq:total-residual}) tailored to this specific example.
(The BCs are omitted from the beginning of this example for simplicity.)
Equations \eqref{eq:least-square-fem}-\eqref{eq:spetral-least-equare} are called least-square finite-element methods (least-square FEMs).
This shows PINNs can be seen as a type of global least-square FEMs.

With the spectral and least-square finite element methods in mind, we can see the analogy between PINNs and conventional numerical methods.
Especially when comparing to the least-square FEMs, their workflows are very close.
The main difference is that PINNs replace the analytical integration with a Monte-Carlo numerical integration.
Further, while the least-square FEMs solve zero-slope conditions to find the minimum residuals, PINNs rely on searching the minimal residuals on a complicated hypersurface of residuals iteratively.
It is due to the complexity and the nonlinearity of the models in PINNs, where neither solving zero-residual conditions on selected points nor solving zero-slope conditions can reduce problems to a linear system. 

The second difference is how to approximate derivatives.
Conventional numerical methods use analytical or numerical differentiation of the models to approximate derivatives.
On the other hand, modern PINNs depend on automatic differentiation due to the complexity and nonlinearity in the networks.

The two differences play an important role in solution accuracy and time-to-solution.
Solving linear systems is intuitively faster than solving a nonlinear optimization problems through 1st-order optimization methods and is cheaper than 2nd-order methods in terms of memory usage.
And while automatic differentiation is powerful, it requires a nontrivial computer memory.
As seen in section \ref{sec:ad}, automatic differentiation needs to memorize all computational graphs for derivatives.
And high-order derivatives needs also to memorize the computational graphs of lower-order derivatives, making the computational graph to grow exponentially.

% vim:ft=tex
