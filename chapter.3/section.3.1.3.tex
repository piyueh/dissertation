%! TEX root = main.tex
In equation \eqref{eq:total-residual-weighted}, each loss term is weighted.
How to properly assign the weights is still an open question.
In references \cite{jin_nsfnets_2020,wang_understanding_2021}, the authors proposed an annealing approach to change these weights adaptively during the optimization process.
However, these adaptive approaches have not been further tested by more works.
In this work, part of the benchmarks will cover the performance of the adaptive weight strategy proposed in \cite{jin_nsfnets_2020}.

Jin et al. \cite{jin_nsfnets_2020} proposed the following annealing loss aggregation algorithm for iterative optimization methods:
\begin{equation}
    r^k(\Theta) = r_{PDE}^k(\Theta) + 
        \left(\left(1-\lambda\right)\alpha^k + \lambda\alpha^{k+1}\right)r_{BC}^k(\Theta) + 
        \left(\left(1-\lambda\right)\beta^k + \lambda\beta^{k+1}\right)r_{IC}^k(\Theta)
\end{equation}
where $k$ denotes the $k$-th iteration in an iterative optimization method.
The subscript $PDE$, $BC$, and $IC$ denote the loss contributed by the residuals of PDEs, BCs, and IC.
$\lambda$ is a user-provided parameter to control the moving average of the current and the previous weights.
The concept of this adaptive approach is to make the gradients of each loss term comparable.
The magnitude of each loss term is reduced at a similar rate when using the gradient-descent method and its derived methods.
And
\begin{equation}
    \alpha^{k+1} = \frac{\overline{\lvert\nabla_\Theta r_{PDE}^k(\Theta)\rvert}}{\overline{\lvert\nabla_\Theta r_{BC}^k(\Theta)\rvert}}
    \text{\ \ \ \ and\ \ \ \ }
    \beta^{k+1} = \frac{\overline{\lvert\nabla_\Theta r_{PDE}^k(\Theta)\rvert}}{\overline{\lvert\nabla_\Theta r_{IC}^k(\Theta)\rvert}}
\end{equation}
$\lvert\cdot\rvert$ denotes the element-wise absolute values of a vector.
$\overline{\lvert\cdot\rvert}$ is the mean of these absolute values.

The following expression better represents the actual implementation in our code:
\begin{equation}
    \begin{aligned}
        &\zeta^k =
            \overline{\lvert\nabla_\Theta r_{m,u}^k(\Theta)\rvert} +
            \overline{\lvert\nabla_\Theta r_{m,v}^k(\Theta)\rvert} +
            \overline{\lvert\nabla_\Theta r_{m,w}^k(\Theta)\rvert} +
            \overline{\lvert\nabla_\Theta r_{c}^k(\Theta)\rvert} \\
        &\alpha^{k+1} = 
            \frac{\zeta^k}{\overline{\lvert\nabla_\Theta r_i^k(\Theta)\rvert}} \\
        &r^k(\Theta) = r_m^k(\Theta) + r_c^k(\Theta) + 
            \sum\limits_{i} \left(\left(1-\lambda\right)\alpha_i^k + \lambda\alpha_i^{k+1}\right) r_i^k(\Theta)
    \end{aligned}
\end{equation}
where $r_{m,u}^k$, $r_{m,v}^k$, and $r_{m,w}^k$ are the $u$-, $v$-, and $w$-component in the residual vector of the momentum equations at $k$-th iteration.
And subscript $i$ denotes different loss terms, excluding $r_m(\Theta)^k$ and $r_c(\Theta)^k$.
In this work, $\lambda$ is fixed at $0.1$ for all cases using annealing loss aggregation.
% vim:ft=tex