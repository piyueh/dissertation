%! TEX program = xelatex
\documentclass[font=Times]{gw-dissertation}[2021/11/19]
\usepackage[style=numeric-comp]{biblatex}  % a modern alternative to bibtex
\usepackage{floatrow}
\usepackage{booktabs} % table separation lines
\usepackage[section]{placeins} % limit floats (figs, tables, etc.) to the same sections
\usepackage[font={singlespacing, footnotesize}, labelfont=bf]{subcaption} % subfigures' captions
\usepackage{listings}
\usepackage{graphicx}
\usepackage[none]{hyphenat}  % disable hyphenation; not required by GW guideline
\usepackage{longtable}
\usepackage{lscape}
\usepackage{tikz} % visual computational graphs
\usepackage{tikzscale}
\usepackage{tikzit}
\usepackage{relsize}
\input{tikzit.tikzstyles}

% bib file location; use only one single bib file
\addbibresource{reference.bib} 

% default figure search path
\graphicspath{{figs}}

% priority of the formats of figures
\DeclareGraphicsExtensions{.pdf,.png}

% load my own macros
\input{macros.tex}

% frontmatter
\input{frontmatter/titlepage}
\input{frontmatter/dedication}
\input{frontmatter/acknowledgment}
\input{frontmatter/abstract}
\input{frontmatter/preface}

% document body
\begin{document}

\input{notion.tex}

\chapter{Introduction}

    \section{Background and Motivation}
    \input{chapter.1/section.1.1.tex}

    \section{Literature Review on Physics-Informed Neural Networks}
    \input{chapter.1/section.1.2.tex}

    \section{Research Aims}
    \input{chapter.1/section.1.3.tex}

\chapter{PetIBM: a distributed-GPU incompressible Navier-Stokes solver}
\input{chapter.2/section.2.1.tex}

    \section{Mathematical Formulation}
    \input{chapter.2/section.2.1.1.tex}

    \section{Implementation Details}
    \input{chapter.2/section.2.1.2.tex}

    \section{Verification and Validation}

    \section{Performance Benchmarks}
    \input{chapter.2/section.2.1.4.tex}

\chapter{Physics-Informed Neural Networks}

    \section{Fundamental of Physics-Informed Neural Networks}

        \subsection{Deep Neural Networks Modeling}
        \input{chapter.3/section.3.1.1.tex}

        \subsection{Periodic Boundaries}\label{section:periodic-boundary}

        \subsection{Remark on difference and similarities with conventional numerical methods}
        \input{chapter.3/section.3.1.2.tex}

        \subsection{Automatic Differentiation}
        \input{chapter.3/section.3.1.3.tex}

        \subsection{Adaptive Loss Reduction}\label{section:loss-annealing}
        \input{chapter.3/section.3.1.4.tex}

    \section{Adam Optimizer, Batched Training, and Cyclical Learning Rates}\label{section:adam}

    \section{Nonlinear Conjugate-Gradient Optimizer and Line Search}

    \section{Stochastic Weight Averaging and Training Strategy}

\chapter{Computational Experiments Configurations and Results}

    \section{2D Taylor-Green Vortex and Configurations}
    \input{chapter.4/section.4.1.tex}

    \section{Vanilla Summation versus Annealing Loss Aggregation}

    \section{Exponentially Decayed versus Cyclical Learning Rates}
    The upper and lower bounds for cycling learning rates are $10^{-3}$ and $10^{-6}$, while one cycle takes 4000 iterations.

    \section{Unsteady Flow Problems}

\chapter{Discussion and Future Works}

\appendix
\chapter{Extra Data and Visualizations for 2D Taylor-Green Vortex Benchmarks}\label{section:extra-data}
\input{appendices/appendix_a.tex}

\end{document}
% vim:ft=tex
